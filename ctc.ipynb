{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# scripts/prepare_manifest.py\n",
        "import os, argparse, json\n",
        "\n",
        "def gather_librispeech(root, out_manifest):\n",
        "    items = []\n",
        "    for subdir, _, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.endswith(\".trans.txt\"):\n",
        "                trans_path = os.path.join(subdir, f)\n",
        "                with open(trans_path, 'r') as fh:\n",
        "                    for line in fh:\n",
        "                        parts = line.strip().split(' ', 1)\n",
        "                        if len(parts) < 2: continue\n",
        "                        key, text = parts\n",
        "                        wav = os.path.join(subdir, key + \".flac\")\n",
        "                        if os.path.exists(wav):\n",
        "                            items.append({\"audio\": wav, \"text\": text.lower()})\n",
        "    with open(out_manifest, 'w') as out:\n",
        "        for it in items:\n",
        "            out.write(json.dumps(it) + \"\\n\")\n",
        "    print(f\"Manifest saved to {out_manifest}, {len(items)} entries.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--librispeech_root\", required=True)\n",
        "    parser.add_argument(\"--out\", default=\"data/manifest.json\")\n",
        "    args = parser.parse_args()\n",
        "    os.makedirs(os.path.dirname(args.out), exist_ok=True)\n",
        "    gather_librispeech(args.librispeech_root, args.out)\n"
      ],
      "metadata": {
        "id": "b1fz1R-ui8qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/features.py\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, sample_rate=16000, n_mels=80, n_fft=512, win_length=400, hop_length=160):\n",
        "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=sample_rate, n_fft=n_fft, win_length=win_length,\n",
        "            hop_length=hop_length, n_mels=n_mels, power=1.0\n",
        "        )\n",
        "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80.0)\n",
        "\n",
        "    def extract(self, waveform):\n",
        "        # waveform: (1, n_samples) torch.Tensor float32\n",
        "        mel = self.mel_spec(waveform)  # (1, n_mels, time)\n",
        "        log_mel = self.amplitude_to_db(mel)\n",
        "        # mean-variance per utterance (CMVN)\n",
        "        mean = log_mel.mean(dim=-1, keepdim=True)\n",
        "        std = log_mel.std(dim=-1, keepdim=True).clamp(min=1e-5)\n",
        "        norm = (log_mel - mean) / std\n",
        "        return norm  # shape: (1, n_mels, time)\n"
      ],
      "metadata": {
        "id": "6mmByKD0jD7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvBranch(nn.Module):\n",
        "    def __init__(self, d_model, expansion=2, kernel_size=31, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.pointwise = nn.Conv1d(d_model, d_model*expansion, 1)\n",
        "        self.depthwise = nn.Conv1d(d_model*expansion, d_model*expansion, kernel_size, padding=kernel_size//2, groups=d_model*expansion)\n",
        "        self.project = nn.Conv1d(d_model*expansion, d_model, 1)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, time, d_model) -> conv expects (batch, d_model, time)\n",
        "        x = x.transpose(1,2)\n",
        "        x = self.pointwise(x)\n",
        "        x = self.depthwise(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.project(x)\n",
        "        x = x.transpose(1,2)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class MultiBranchLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, conv_expansion=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.conv_branch = ConvBranch(d_model, expansion=conv_expansion, dropout=dropout)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model*4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model*4, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # attention path\n",
        "        attn_out,_ = self.self_attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        # conv branch\n",
        "        conv_out = self.conv_branch(x)\n",
        "        x = self.norm2(x + conv_out)\n",
        "        # ff\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm3(x + ff_out)\n",
        "        return x\n",
        "\n",
        "class EBranchformerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=80, d_model=256, num_layers=12, nhead=4, conv_expansion=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.layers = nn.ModuleList([MultiBranchLayer(d_model, nhead, conv_expansion, dropout) for _ in range(num_layers)])\n",
        "        self.final_ln = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        # x: (batch, n_mels, time) -> transpose to (batch, time, n_mels)\n",
        "        x = x.squeeze(1).transpose(1,2) if x.dim()==3 else x.transpose(1,2)\n",
        "        x = self.input_proj(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.final_ln(x)\n",
        "        return x  # (batch, time, d_model)\n"
      ],
      "metadata": {
        "id": "uWdBK-cMjHS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/train.py\n",
        "import os, json, yaml, argparse, math\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "from src.features import FeatureExtractor\n",
        "from src.model import EBranchformerEncoder\n",
        "\n",
        "class LibriDataset(Dataset):\n",
        "    def __init__(self, manifest, tokenizer, feat_extractor):\n",
        "        self.items = []\n",
        "        with open(manifest) as fh:\n",
        "            for line in fh:\n",
        "                self.items.append(json.loads(line.strip()))\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feat_extractor = feat_extractor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        it = self.items[idx]\n",
        "        wav, sr = torchaudio.load(it['audio'])\n",
        "        if wav.shape[0] > 1:\n",
        "            wav = wav.mean(dim=0, keepdim=True)\n",
        "        feats = self.feat_extractor.extract(wav)  # (1, n_mels, time)\n",
        "        text = it['text']\n",
        "        target = torch.tensor(self.tokenizer.encode(text), dtype=torch.long)\n",
        "        return feats, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    feats = [b[0] for b in batch]\n",
        "    targets = [b[1] for b in batch]\n",
        "    # pad feats on time dim\n",
        "    max_t = max(f.shape[-1] for f in feats)\n",
        "    feats_p = []\n",
        "    input_lengths = []\n",
        "    for f in feats:\n",
        "        pad = max_t - f.shape[-1]\n",
        "        if pad>0:\n",
        "            f = torch.nn.functional.pad(f, (0,pad))\n",
        "        feats_p.append(f)\n",
        "        input_lengths.append(f.shape[-1])\n",
        "    feats_p = torch.stack(feats_p)  # (B, 1, n_mels, T)\n",
        "    targets_concat = torch.cat(targets)\n",
        "    target_lengths = torch.tensor([t.shape[0] for t in targets], dtype=torch.long)\n",
        "    return feats_p, targets_concat, torch.tensor(input_lengths, dtype=torch.long), target_lengths\n",
        "\n",
        "# Simple tokenizer (char-level)\n",
        "class CharTokenizer:\n",
        "    def __init__(self):\n",
        "        chars = [\"'\", \" \"] + [chr(i) for i in range(97, 123)]  # a-z plus apostrophe and space\n",
        "        self.vocab = ['<blank>'] + chars\n",
        "        self.ctoi = {c:i for i,c in enumerate(self.vocab)}\n",
        "    def encode(self, text):\n",
        "        text = text.lower()\n",
        "        return [self.ctoi.get(ch, self.ctoi[\" \"]) for ch in text]\n",
        "    def decode(self, ids):\n",
        "        return ''.join([self.vocab[i] for i in ids if i!=0])\n",
        "\n",
        "def train(manifest, config_path, out_dir):\n",
        "    config = yaml.safe_load(open(config_path))\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    device = torch.device(config['train'].get('device','cuda') if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    feat = FeatureExtractor(**config['dataset'])\n",
        "    tokenizer = CharTokenizer()\n",
        "    ds = LibriDataset(manifest, tokenizer, feat)\n",
        "    dl = DataLoader(ds, batch_size=config['train']['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
        "\n",
        "    model_enc = EBranchformerEncoder(input_dim=config['dataset']['n_mels'],\n",
        "                                     d_model=config['model']['d_model'],\n",
        "                                     num_layers=config['model']['num_layers'],\n",
        "                                     nhead=config['model']['nhead'],\n",
        "                                     conv_expansion=config['model']['conv_expansion'],\n",
        "                                     dropout=config['model']['dropout']).to(device)\n",
        "    ctc_head = nn.Linear(config['model']['d_model'], len(tokenizer.vocab)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(list(model_enc.parameters()) + list(ctc_head.parameters()), lr=config['train']['lr'])\n",
        "    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "\n",
        "    for epoch in range(config['train']['epochs']):\n",
        "        model_enc.train()\n",
        "        epoch_loss = 0.0\n",
        "        pbar = tqdm(dl, desc=f\"Epoch {epoch+1}/{config['train']['epochs']}\")\n",
        "        for feats_p, targets_concat, input_lengths, target_lengths in pbar:\n",
        "            feats_p = feats_p.to(device)  # (B,1,n_mels,T)\n",
        "            B, _, _, T = feats_p.size()\n",
        "            feats_p = feats_p.squeeze(1)  # (B, n_mels, T)\n",
        "            enc_out = model_enc(feats_p.to(device))  # (B, T, d_model)\n",
        "            logits = ctc_head(enc_out)  # (B, T, C)\n",
        "            log_probs = logits.log_softmax(-1).permute(1,0,2)  # (T, B, C)\n",
        "            input_lengths = torch.tensor(input_lengths, dtype=torch.long)\n",
        "            # convert input_lengths from number of frames already\n",
        "            loss = ctc_loss(log_probs, targets_concat.to(device), input_lengths, target_lengths.to(device))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(list(model_enc.parameters())+list(ctc_head.parameters()), 5.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "        avg_loss = epoch_loss / len(dl)\n",
        "        print(f\"Epoch {epoch+1} finished. Avg Loss: {avg_loss:.4f}\")\n",
        "        torch.save({\n",
        "            'epoch': epoch+1,\n",
        "            'model_state': model_enc.state_dict(),\n",
        "            'ctc_head': ctc_head.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }, os.path.join(out_dir, f'checkpoint_epoch{epoch+1}.pt'))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--manifest\", required=True)\n",
        "    parser.add_argument(\"--config\", default=\"configs/config.yaml\")\n",
        "    parser.add_argument(\"--out\", default=\"checkpoints\")\n",
        "    args = parser.parse_args()\n",
        "    train(args.manifest, args.config, args.out)\n"
      ],
      "metadata": {
        "id": "0-nj5rwijLU7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}